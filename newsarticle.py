# -*- coding: utf-8 -*-
"""newsarticle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RxvWUDVIDvKr04vodLAa2M3HFo2Z-bia
"""

from bs4 import BeautifulSoup
import requests
import pandas as pd

url="https://www.thestar.com.my/news/latest/"

r=requests.get(url)
soup=BeautifulSoup(r.text,"html.parser")

url_tag =soup.find("url",class_="pager")

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import dateutil.parser
import pytz
import pandas as pd
from dateutil import parser

# Define the function to parse timestamps
def parse_timestamp(timestamp_str):
    try:
        # Try to parse the timestamp using dateutil.parser
        timestamp = parser.parse(timestamp_str)
        # Convert to Indian Standard Time (IST)
        timestamp = timestamp.replace(tzinfo=pytz.utc).astimezone(pytz.timezone("Asia/Kolkata"))
        return timestamp.strftime("%Y-%m-%d %H:%M:%S")
    except ValueError:
        # If parsing fails, handle relative time
        if 'm' in timestamp_str:
            minutes_ago = int(timestamp_str.split('m')[0])
            return (datetime.now(pytz.timezone("Asia/Kolkata")) - timedelta(minutes=minutes_ago)).strftime("%Y-%m-%d %H:%M:%S")
        elif 'h' in timestamp_str:
            hours_ago = int(timestamp_str.split('h')[0])
            return (datetime.now(pytz.timezone("Asia/Kolkata")) - timedelta(hours=hours_ago)).strftime("%Y-%m-%d %H:%M:%S")

# Define the URL
url = "https://www.thestar.com.my/news/latest/"

# Initialize lists to store data
data_content_titles = []
timestamps = []
summary_texts = []

# Loop through pages to scrape data
for i in range(1, 2):
    page_url = f"{url}?pgno={i}#Latest"
    print("Page URL:", page_url)
    r = requests.get(page_url)
    soup = BeautifulSoup(r.text, "html.parser")

    h2_tags = soup.find_all("h2", class_="f18")
    for h2_tag in h2_tags:
        a_tag = h2_tag.find("a")
        if a_tag:
            data_content_title = a_tag.get("data-content-title")
            data_content_titles.append(data_content_title)
        else:
            print("No <a> tag found inside <h2> with class 'f18'.")

    date_tags = soup.find_all("div", class_="col-xs-4 col-sm-2 tm-time")
    for date_tag in date_tags:
        time_tag = date_tag.find("time", class_="timestamp")
        if time_tag:
            timestamp_str = time_tag.get_text(strip=True)
            timestamp = parse_timestamp(timestamp_str)

            if timestamp:
                timestamps.append(timestamp)

                div_tag = date_tag.find_next("div", class_="timeline-content")
                p_tag = div_tag.find("p")

                if p_tag:
                    summary_text = p_tag.get_text(strip=True)
                    summary_texts.append(summary_text)
                else:
                    print("No <p> tag found inside <div>.")
        else:
            print("No <time> tag with class 'timestamp' found inside <div>.")

# Ensure all lists have the same length
min_length = min(len(data_content_titles), len(timestamps), len(summary_texts))
data_content_titles = data_content_titles[:min_length]
timestamps = timestamps[:min_length]
summary_texts = summary_texts[:min_length]

# Create the DataFrame
news = pd.DataFrame({
    "Title": data_content_titles,
    "Timestamp": timestamps,
    "Content": summary_texts
})

# Convert the 'Timestamp' column to a pandas datetime object
news['Timestamp'] = pd.to_datetime(news['Timestamp'])

# Create 'Date' and 'Time' columns
news['Date'] = news['Timestamp'].dt.date
news['Time'] = news['Timestamp'].dt.time

# Rearrange the columns if needed
news = news[['Title', 'Date', 'Time', 'Content']]

# Print the final DataFrame
news

news.Content

"""Data CLEANING"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
import string

# Download NLTK resources (run only once)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def clean_text(text):
    # Step 1: Convert text to lowercase
    text = text.lower()

    # Step 2: Tokenization
    tokens = word_tokenize(text)

    # Step 3: Removing punctuation
    tokens = [token for token in tokens if token not in string.punctuation]

    # Step 4: Removing special characters
    tokens = [token for token in tokens if token.isalnum()]

    # Step 5: Removing stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    # Step 6: Stemming and Lemmatization
    porter = PorterStemmer()
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Join tokens back into text
    cleaned_text = ' '.join(lemmatized_tokens)

    return cleaned_text

# Apply the cleaning function to the 'Content' column
news['Clean_Article'] = news['Content'].apply(clean_text)

print(news)

"""#TEXT SUMMARIZATION"""

from transformers import BartForConditionalGeneration, BartTokenizer

# Load pre-trained model and tokenizer
model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")
tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")

# Define a function to generate summaries
def generate_summary(text):
    inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(inputs, num_beams=4, min_length=30, max_length=200, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Apply the generate_summary function to the 'Cleaned_Summary' column
news['Summarize'] = news['Clean_Article'].apply(generate_summary)

# Print the DataFrame with summaries
print(news[['Title', 'Date', 'Time', 'Summarize']])

# Create a new DataFrame with selected columns
clean_data = news[['Title', 'Date', 'Time', 'Summarize']]
print("New DataFrame with Selected Columns:")
print(clean_data)

news.dtypes

news.Summarize

!pip install summa

"""generated title

"""

'''from summa import keywords

# Sample summaries (replace this with your actual data)
summaries = news.Cleaned_Content
# Generate titles based on keyword extraction using TextRank
for i, summary in enumerate(summaries):
    # Extract keywords using TextRank
    summary_keywords = keywords.keywords(summary)

    # Generate title using the extracted keywords
    # Example: Take the first few words of the summary and append the extracted keywords
    summary_words = summary.split()
    title = ' '.join(summary_words[:5]) + f" - {summary_keywords}"
    print(f"Generated Title: {title}")
title'''

import pandas as pd
from summa import keywords

# Sample summaries (replace this with your actual data)
summaries = news['Clean_Article']

# Create an empty DataFrame to store the titles
titles_df = pd.DataFrame(columns=['Generated_Title'])

# Generate titles based on keyword extraction using TextRank
for summary in summaries:
    # Extract keywords using TextRank
    summary_keywords = keywords.keywords(summary)

    # Generate title using the extracted keywords
    # Example: Take the first few words of the summary and append the extracted keywords
    summary_words = summary.split()
    title = ' '.join(summary_words[:5]) + f" - {summary_keywords}"

    # Append the title to the DataFrame
    titles_df = titles_df.append({'Generated_Title': title}, ignore_index=True)

# Display the new DataFrame with generated titles
print("DataFrame with Generated Titles:",titles_df)
#print(titles_df)

# Concatenate the DataFrames along the columns axis
merged_df = pd.concat([news, titles_df], axis=1)

# Display the merged DataFrame
print("Merged DataFrame:")
print(merged_df)
merged_df.to_csv('summary_column.csv', index=False, header=['Title', 'Date', 'Time','Content','Clean_Article','Summarize','Generated_Title'])

merged_df.dtypes

merged_df.duplicated().sum()

merged_df.columns

"""#Topic Modeling"""

from google.colab import files

# Download the CSV file
files.download('cleandata.csv')

import gensim
from gensim import corpora
from gensim.models import LdaModel
import numpy as np

# Sample documents for topic modeling
documents = merged_df.Summarize
# Tokenize words in documents
tokenized_docs = [doc.split() for doc in documents]

# Create dictionary and corpus
dictionary = corpora.Dictionary(tokenized_docs)
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]

# Build LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                            id2word=dictionary,
                                            num_topics=2,
                                            random_state=42,
                                            update_every=1,
                                            passes=10,
                                            alpha='symmetric',
                                            per_word_topics=True)

# Calculate log likelihood
log_likelihood = lda_model.log_perplexity(corpus)

# Calculate word count
word_count = sum(cnt for document in corpus for _, cnt in document)

# Calculate perplexity using the formula
perplexity = np.exp2(-log_likelihood / word_count)

print("Perplexity:", perplexity)

import gensim
from gensim import corpora
from pprint import pprint
from gensim.models import CoherenceModel
import numpy as np

# Sample documents for topic modeling
documents = merged_df.Summarize
# Tokenize words in documents
tokenized_docs = [doc.split() for doc in documents]

# Create dictionary and corpus
dictionary = corpora.Dictionary(tokenized_docs)
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]

# Define a function to compute coherence score for a given LDA model
def compute_coherence_values(dictionary, corpus, texts, num_topics, alpha, passes):
    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                                id2word=dictionary,
                                                num_topics=num_topics,
                                                alpha=alpha,
                                                passes=passes)
    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')
    return coherence_model_lda.get_coherence()

# Define a grid of parameters to search through
grid = {}
grid['num_topics'] = [4]  # You can add more values to search through
grid['alpha'] = ['symmetric', 'asymmetric']
grid['passes'] = [5]

# Perform grid search
best_score = -1
best_params = {}
for num_topics in grid['num_topics']:
    for alpha in grid['alpha']:
        for passes in grid['passes']:
            coherence_score = compute_coherence_values(dictionary, corpus, tokenized_docs, num_topics, alpha, passes)
            if coherence_score > best_score:
                best_score = coherence_score
                best_params['num_topics'] = num_topics
                best_params['alpha'] = alpha
                best_params['passes'] = passes

print("Best parameters:", best_params)
print("Best coherence score:", best_score)
# Calculate log likelihood
log_likelihood = lda_model.log_perplexity(corpus)
# Calculate word count
word_count = sum(cnt for document in corpus for _, cnt in document)
# Calculate perplexity using the formula
perplexity = np.exp2(-log_likelihood / word_count)
print("Perplexity:", perplexity)

merged_df.dtypes

"""#text classification"""

import pandas as pd
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

# Download NLTK resources (if not already downloaded)
nltk.download('vader_lexicon')

# Initialize the SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()

# Sample DataFrame (replace this with your actual DataFrame)
data = merged_df.Summarize
merged_df = pd.DataFrame(data)

# Analyze sentiment for each element in the DataFrame and store results
sentiments = []
for text in merged_df['Summarize']:
    sentiment_scores = sid.polarity_scores(text)
    sentiments.append(sentiment_scores)

# Add sentiment scores to the DataFrame
merged_df['Sentiment'] = sentiments

# Determine if news is hot based on compound sentiment score
# You can define your own threshold for what constitutes hot news
hot_threshold = 0.5

# Check if the compound sentiment score exceeds the hot threshold
merged_df['Hot'] = merged_df['Sentiment'].apply(lambda x: x['compound'] > hot_threshold)

# Print the DataFrame with hot news identified
print(merged_df) 




#aws bucket
import boto3
import pandas as pd
s3 = boto3.resource(
     service_name = 's3',
     region_name = 'ap-south-1',
     aws_access_key_id = 'AKIA47CRVKQASQQGBRGW',
     aws_secret_access_key = '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')

for bucket in s3.buckets.all():
    print(bucket.name)
import os

os.environ["AWS_DEFAULT_REGION"] = 'ap-south-1'
os.environ["AWS_ACCESS_KEY_ID"] = 'AKIA47CRVKQASQQGBRGW'
os.environ["AWS_SECRET_ACCESS_KEY"] = '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'

s3.Bucket('himani1').upload_file(Filename='summary_column.csv',Key='summary_column.csv')
for obj in s3.Bucket('himani1').objects.all():
    print(obj)
s3.Bucket('himani1').Object('summary_column.csv').get()

























